import streamlit as st
from langgraph.graph import END, START, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

from state import GraphState
from nodes import (
    RetrieveNode,
    QuestionRouterNode,
    WebSearchNode,
    InsurancePolicyRetrieveNode,
    LLMAiAnswerNode,
    LLMRagAnswerNode,
    LLMRagPdfAnswerNode,
)


def create_graph():
    workflow = StateGraph(GraphState)

    # ë…¸ë“œ ë“±ë¡
    workflow.add_node("web_search", WebSearchNode())
    workflow.add_node("retrieve", RetrieveNode())
    workflow.add_node("insurance_policy_retrieve", InsurancePolicyRetrieveNode())
    workflow.add_node("llm_answer_rag", LLMRagAnswerNode())
    workflow.add_node("llm_answer_ai_model", LLMAiAnswerNode())
    workflow.add_node("llm_answer_rag_pdf", LLMRagPdfAnswerNode())

    # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        START,
        QuestionRouterNode(),
        {
            "web_search": "web_search",
            "retrieve": "retrieve",
            "insurance_policy_retrieve": "insurance_policy_retrieve",
        },
    )

    # ë…¸ë“œ ê°„ ì—°ê²°
    workflow.add_edge("web_search", "llm_answer_rag")
    workflow.add_edge("insurance_policy_retrieve", "llm_answer_rag_pdf")
    workflow.add_edge("retrieve", "llm_answer_ai_model")

    # ì¢…ë£Œ ì„¤ì •
    workflow.add_edge("llm_answer_rag", END)
    workflow.add_edge("llm_answer_ai_model", END)
    workflow.add_edge("llm_answer_rag_pdf", END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app = workflow.compile(checkpointer=MemorySaver())
    return app


def stream_graph(
    app,
    query: str,
    streamlit_container,
    thread_id: str,
):

    config = RunnableConfig(recursion_limit=5, configurable={"thread_id": thread_id})

    inputs = GraphState(question=query)

    actions = {
        "retrieve": "ğŸ” ê³ ê° ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
        "llm_answer_ai_model": "ğŸ‘¨ğŸ»â€ğŸ’» ê³ ê° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
        "insurance_policy_retrieve": "ğŸ” ë¬¸ì„œ ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
        "llm_answer_rag_pdf": "ğŸ‘¨ğŸ»â€ğŸ’» ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
        "web_search": "ğŸ›œ ì›¹ ê²€ìƒ‰ì„ ì§„í–‰í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
        "llm_answer_rag": "ğŸ‘¨ğŸ»â€ğŸ’» ì›¹ ê²€ìƒ‰ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
    }

    with streamlit_container.status(
        "ì—´ì‹¬íˆ ë¶„ì„ì¤‘ì…ë‹ˆë‹¤..ğŸ‘¨ğŸ»â€ğŸ’»", expanded=True
    ) as status:
        st.write("ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..ğŸ§‘â€ğŸ’» ")
        for output in app.stream(inputs, config=config):
            for key, value in output.items():
                if key in actions:
                    st.write(actions[key])
        status.update(label="ë‹µë³€ ì™„ë£Œ", state="complete", expanded=False)

    return app.get_state(config={"configurable": {"thread_id": thread_id}}).values
